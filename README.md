# Evading-DeepFake-Detectors-via-Conditional-Diffusion-Models


This repository provides the official PyTorch implementation of our paper:

> **Evading DeepFake Detectors via Conditional Diffusion Models**  
> Wenhao Wang, Fangjun Huang  
> _IH&MMSec 2024, ACM Workshop on Information Hiding and Multimedia Security_  
> [[Paper (PDF)]](./01è®ºæ–‡pdfç‰ˆæœ¬.pdf) | [[ACM DOI]](https://doi.org/10.1145/3658664.3659653)

## ğŸ§  Overview

DeepFake detectors play a critical role in identifying manipulated facial images. However, they are vulnerable to adversarial attacks. This work proposes a **semantic reconstruction-based adversarial attack** that leverages conditional diffusion models to generate imperceptible perturbations in the latent space.

![](./assets/framework.png)

### Key Highlights

- ğŸŒ€ **Latent space attack** using DDIM-guided conditional sampling.
- ğŸ•µï¸ **Evades both spatial and frequency domain detectors** (e.g., XceptionNet, SRM).
- ğŸ–¼ï¸ **High visual quality** â€” minimal artifacts, low LPIPS, and low FID.
- ğŸ¯ Supports white-box and black-box settings with **meta-learning transferability**.

---

## ğŸ”§ Method

The proposed attack optimizes the latent representation of fake images in a semantic latent space extracted by a diffusion autoencoder. A conditional DDIM decoder reconstructs adversarial faces guided by:

- **Attack loss**: Cross-entropy to mislead the detector.
- **Perceptual loss**: LPIPS + L2 to preserve structure and realism.
